# 安装Ceph

## 环境说明

* CentOS 7.5 x64 minimal；
* 3台虚拟机配置16G内存、4vCPU，每台1个40G硬盘安装系统，3个10G硬盘用于ceph osd；
* ens32通连接到互联网，作为public网络，ens34-cls作为cluster network；
* ceph-client节点复用做deploy节点，配置为4vCPU，16G内存，1个40G硬盘安装系统。

## 修改网络配置

| 主机名 |     ens32     |     ens34-cls      |
| :----: | :------------: | :------------: |
| ceph01 | 172.16.16.81/24 | 172.16.2.20/24 |
| ceph02 | 172.16.16.82/24 | 172.16.2.21/24 |
| ceph03 | 172.16.16.83/24 | 172.16.2.22/24 |
| ceph-client | 172.16.16.84/24 | 172.16.2.22/24 |

* 根据以上信息配置网络；
* 设置主机名时，一定不要使用大写，后续ceph-deploy创建mon时会出错。

```bash
# vi /etc/hosts
172.16.16.81     ceph01
172.16.16.82     ceph02
172.16.16.83     ceph03
172.16.16.84     ceph04
172.16.160.81     ceph01-cls
172.16.160.82     ceph02-cls
172.16.160.83     ceph03-cls
172.16.160.84     ceph04-cls
```

## 配置源

```bash
# cd /etc/yum.repos.d
# rm *.repo
# vi CentOS.repo
[base]
name=CentOS-$releasever - Base
baseurl=https://mirrors.aliyun.com/centos/$releasever/os/$basearch/
gpgcheck=1
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7

[updates]
name=CentOS-$releasever - Updates
baseurl=https://mirrors.aliyun.com/centos/$releasever/updates/$basearch/
gpgcheck=1
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7

[extras]
name=CentOS-$releasever - Extras
baseurl=https://mirrors.aliyun.com/centos/$releasever/extras/$basearch/
gpgcheck=1
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7

[centosplus]
name=CentOS-$releasever - Plus
baseurl=https://mirrors.aliyun.com/centos/$releasever/centosplus/$basearch/
gpgcheck=1
enabled=0
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7

[centos-ceph-luminous]
name=CentOS-$releasever - Ceph Luminous
baseurl=https://mirrors.aliyun.com/centos/$releasever/storage/$basearch/ceph-luminous/
gpgcheck=0
enabled=1
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-SIG-Storage

[Ceph-noarch]
name=Ceph noarch packages
baseurl=http://download.ceph.com/rpm-luminous/el7/noarch
enabled=1
gpgcheck=1
type=rpm-md
gpgkey=https://download.ceph.com/keys/release.asc

[epel]
name=Extra Packages for Enterprise Linux 7 - $basearch
baseurl=http://mirrors.aliyun.com/epel/7/$basearch
failovermethod=priority
enabled=1
gpgcheck=0
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7
# yum update
```

* 配置CentOS源、Ceph源和EPEL源。

## 修改系统环境

```bash
# yum install "@Compatibility Libraries" "@Development Tools" net-tools wget bash-completion chrony python-pip
# pip install distribute
# systemctl enable chronyd
# systemctl start chronyd
# timedatectl set-timezone Asia/Shanghai
# vi /etc/sysconfig/selinux
SELINUX=disabled
# setenforce 0
```

* CentOS minimal 7.5还需安装开发工具、开发库、chronyd时间服务，python-pip等包；
* python-pip安装distribute包，否则ceph-deploy无法执行；
* 启动chronyd时间服务、设置时区，并关闭selinux。

## 创建sudo用户

```bash
# groupadd -g 500 ceph
# useradd -g 500 -u 500 ceph
# passwd ceph
Changing password for user ceph.
New password:
BAD PASSWORD: The password is shorter than 8 characters
Retype new password:
passwd: all authentication tokens updated successfully.
# echo "ceph ALL = (root) NOPASSWD:ALL" | sudo tee /etc/sudoers.d/ceph
ceph ALL = (root) NOPASSWD:ALL
# chmod 0440 /etc/sudoers.d/ceph
```

* 每个节点都创建用户用于日常管理；
* 配置sudo免密码，建议配置完后测试用户使用sudo执行需要系统特权的命令；
* 配置完sudo后，后续将不适用root用户管理。

## 配置ssh互信

```bash
[ceph@ceph-client ~]$ ssh-keygen
[ceph@ceph-client ~]$ ssh-copy-id ceph@ceph01
[ceph@ceph-client ~]$ ssh-copy-id ceph@ceph02
[ceph@ceph-client ~]$ ssh-copy-id ceph@ceph03
[ceph@ceph-client ~]$ ssh-copy-id ceph@ceph-client
```

* 创建密钥，并通过集群心跳网络传输给其他节点；
* 分发完成后，测试能否通过ssh免密码直接访问其他节点。

## 配置firewalld

```bash
# sudo firewall-cmd --zone=public --add-service=ceph-mon --permanent
# sudo firewall-cmd --zone=public --add-service=ceph --permanent
# sudo firewall-cmd --reload
```

## 安装ceph-deploy

```bash
[ceph@ceph-client ~]$ sudo yum install ceph-deploy
```

## 创建集群

```bash
[ceph@ceph-client ~]$ mkdir cluster
[ceph@ceph-client ~]$ cd cluster/
[ceph@ceph-client cluster]$ ceph-deploy new --cluster-network 172.16.160.0/24 --public-network 172.16.16.0/24 ceph01
```

* 创建目录并在目录中执行ceph-deploy，可以将部署相关配置、keyring等文件保存在该目录。

```bash
[ceph@ceph-client cluster]$  ceph-deploy install ceph01 ceph02 ceph03 ceph-client --no-adjust-repos
```

* 通过ceph-deploy在ceph01、ceph02、ceph03节点上安装软件，由于我们已经配置好yum源，因此无需让ceph-deploy修改安装源。

```bash
[ceph@ceph-client cluster]$  ceph-deploy mon create-initial
[ceph@ceph-client cluster]$  ceph-deploy admin ceph01 ceph02 ceph03 ceph-client
[ceph@ceph-client cluster]$  ceph-deploy mgr create ceph01 ceph02 ceph03
[ceph@ceph-client cluster]$  ceph-deploy osd create --data /dev/sdb ceph01
[ceph@ceph-client cluster]$  ceph-deploy osd create --data /dev/sdb ceph02
[ceph@ceph-client cluster]$  ceph-deploy osd create --data /dev/sdb ceph03
[ceph@ceph-client cluster]$  ceph-deploy osd create --data /dev/sdc ceph01
[ceph@ceph-client cluster]$  ceph-deploy osd create --data /dev/sdc ceph02
[ceph@ceph-client cluster]$  ceph-deploy osd create --data /dev/sdc ceph03
[ceph@ceph-client cluster]$  ceph-deploy osd create --data /dev/sdd ceph01
[ceph@ceph-client cluster]$  ceph-deploy osd create --data /dev/sdd ceph02
[ceph@ceph-client cluster]$  ceph-deploy osd create --data /dev/sdd ceph03
```

* 最小集群需要一个mon和若干osd，luminous以上版本还需要mgr；
* 先于ceph01节点上创建mon；
* 通过ceph-deploy admin将配置和key分发到其他节点；
* 在ceph01、ceph02、ceph03节点上创建mgr；
* 在ceph01、ceph02、ceph03节点上每个磁盘上创建一个osd进程。

## 检查集群状态

```bash
[ceph@ceph-client cluster]$  sudo ceph health
HEALTH_OK
[ceph@ceph-client cluster]$ sudo ceph health
HEALTH_OK
[ceph@ceph-client cluster]$ sudo ceph -s
  cluster:
    id:     870de175-7f08-4931-a96e-3b8b9ba83484
    health: HEALTH_OK

  services:
    mon: 1 daemons, quorum ceph01
    mgr: ceph01(active), standbys: ceph02, ceph03
    osd: 9 osds: 9 up, 9 in

  data:
    pools:   0 pools, 0 pgs
    objects: 0 objects, 0 bytes
    usage:   9519 MB used, 82604 MB / 92124 MB avail
    pgs:
```

## 扩展集群

```bash
[ceph@ceph-client cluster]$  ceph-deploy mds create ceph01 ceph02 ceph03
```

* 创建mds用于cephfs。

```bash
[ceph@ceph-client cluster]$  ceph-deploy mon add ceph02
[ceph@ceph-client cluster]$  ceph-deploy mon add ceph03
```

* 在其他节点上增加mon。

```bash
[ceph@ceph-client cluster]$ sudo ceph quorum_status --format json-pretty
{
    "election_epoch": 12,
    "quorum": [
        0,
        1,
        2
    ],
    "quorum_names": [
        "ceph01",
        "ceph02",
        "ceph03"
    ],
    "quorum_leader_name": "ceph01",
    "monmap": {
        "epoch": 3,
        "fsid": "870de175-7f08-4931-a96e-3b8b9ba83484",
        "modified": "2018-05-17 22:48:12.798052",
        "created": "2018-05-17 22:41:24.227607",
        "features": {
            "persistent": [
                "kraken",
                "luminous"
            ],
            "optional": []
        },
        "mons": [
            {
                "rank": 0,
                "name": "ceph01",
                "addr": "172.16.16.81:6789/0",
                "public_addr": "172.16.16.81:6789/0"
            },
            {
                "rank": 1,
                "name": "ceph02",
                "addr": "172.16.16.82:6789/0",
                "public_addr": "172.16.16.82:6789/0"
            },
            {
                "rank": 2,
                "name": "ceph03",
                "addr": "172.16.16.83:6789/0",
                "public_addr": "172.16.16.83:6789/0"
            }
        ]
    }
}
```

* 检查集群的仲裁信息。

```bash
[ceph@ceph-client cluster]$ sudo ceph -s
  cluster:
    id:     870de175-7f08-4931-a96e-3b8b9ba83484
    health: HEALTH_OK

  services:
    mon: 3 daemons, quorum ceph01,ceph02,ceph03
    mgr: ceph01(active), standbys: ceph02, ceph03
    osd: 9 osds: 9 up, 9 in

  data:
    pools:   0 pools, 0 pgs
    objects: 0 objects, 0 bytes
    usage:   9519 MB used, 82604 MB / 92124 MB avail
    pgs:
```

* mgr为active-standby模式；
* mon共3个，状态为quorum。

```bash
[ceph@ceph-client cluster]$  ceph-deploy rgw create ceph01 ceph02 ceph03
[ceph@ceph-client cluster]$ sudo ceph -s
  cluster:
    id:     870de175-7f08-4931-a96e-3b8b9ba83484
    health: HEALTH_WARN
            too few PGs per OSD (10 < min 30)

  services:
    mon: 3 daemons, quorum ceph01,ceph02,ceph03
    mgr: ceph01(active), standbys: ceph02, ceph03
    osd: 9 osds: 9 up, 9 in
    rgw: 3 daemons active

  data:
    pools:   4 pools, 32 pgs
    objects: 187 objects, 1113 bytes
    usage:   9530 MB used, 82593 MB / 92124 MB avail
    pgs:     32 active+clean
```

* 创建rgw使用对象存储。

```bash
[ceph@ceph-client cluster]$ sudo ceph osd pool create rbd 256 256
pool 'rbd' created
[ceph@ceph-client cluster]$ sudo rbd pool init rbd
[ceph@ceph-client cluster]$ sudo ceph -s
  cluster:
    id:     870de175-7f08-4931-a96e-3b8b9ba83484
    health: HEALTH_OK

  services:
    mon: 3 daemons, quorum ceph01,ceph02,ceph03
    mgr: ceph01(active), standbys: ceph02, ceph03
    osd: 9 osds: 9 up, 9 in
    rgw: 3 daemons active

  data:
    pools:   5 pools, 288 pgs
    objects: 187 objects, 1113 bytes
    usage:   9543 MB used, 82580 MB / 92124 MB avail
    pgs:     288 active+clean
```

* 创建rbd池，将pg和pgp设置为256，消除告警。
* 也可以创建rbd池后，通过`# ceph osd pool set rbd pg_num 256`和`# ceph osd pool set rbd pgp_num 256`修改参数消除告警。