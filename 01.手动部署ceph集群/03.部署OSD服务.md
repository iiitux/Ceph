# 部署OSD服务

## 部署Bluestore OSD

### Ceph01节点部署OSD

```bash
[root@ceph01 ~]# ceph-volume lvm create --data /dev/sdb
Running command: /bin/ceph-authtool --gen-print-key
Running command: /bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring -i - osd new f2440eea-04af-4167-944e-f1a833273412
Running command: vgcreate --force --yes ceph-a177bde5-dfb4-4f4d-8880-ad7dbbe06e0c /dev/sdb
 stdout: Physical volume "/dev/sdb" successfully created.
 stdout: Volume group "ceph-a177bde5-dfb4-4f4d-8880-ad7dbbe06e0c" successfully created
Running command: lvcreate --yes -l 100%FREE -n osd-block-f2440eea-04af-4167-944e-f1a833273412 ceph-a177bde5-dfb4-4f4d-8880-ad7dbbe06e0c
 stdout: Logical volume "osd-block-f2440eea-04af-4167-944e-f1a833273412" created.
Running command: /bin/ceph-authtool --gen-print-key
Running command: mount -t tmpfs tmpfs /var/lib/ceph/osd/ceph-0
Running command: chown -R ceph:ceph /dev/dm-0
Running command: ln -s /dev/ceph-a177bde5-dfb4-4f4d-8880-ad7dbbe06e0c/osd-block-f2440eea-04af-4167-944e-f1a833273412 /var/lib/ceph/osd/ceph-0/block
Running command: ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring mon getmap -o /var/lib/ceph/osd/ceph-0/activate.monmap
 stderr: got monmap epoch 3
Running command: ceph-authtool /var/lib/ceph/osd/ceph-0/keyring --create-keyring --name osd.0 --add-key AQD19VVbuAryCxAAChomHp4Kfw0VkSge4bMH/A==
 stdout: creating /var/lib/ceph/osd/ceph-0/keyring
added entity osd.0 auth auth(auid = 18446744073709551615 key=AQD19VVbuAryCxAAChomHp4Kfw0VkSge4bMH/A== with 0 caps)
Running command: chown -R ceph:ceph /var/lib/ceph/osd/ceph-0/keyring
Running command: chown -R ceph:ceph /var/lib/ceph/osd/ceph-0/
Running command: /bin/ceph-osd --cluster ceph --osd-objectstore bluestore --mkfs -i 0 --monmap /var/lib/ceph/osd/ceph-0/activate.monmap --keyfile - --osd-data /var/lib/ceph/osd/ceph-0/ --osd-uuid f2440eea-04af-4167-944e-f1a833273412 --setuser ceph --setgroup ceph
--> ceph-volume lvm prepare successful for: /dev/sdb
Running command: ceph-bluestore-tool --cluster=ceph prime-osd-dir --dev /dev/ceph-a177bde5-dfb4-4f4d-8880-ad7dbbe06e0c/osd-block-f2440eea-04af-4167-944e-f1a833273412 --path /var/lib/ceph/osd/ceph-0
Running command: ln -snf /dev/ceph-a177bde5-dfb4-4f4d-8880-ad7dbbe06e0c/osd-block-f2440eea-04af-4167-944e-f1a833273412 /var/lib/ceph/osd/ceph-0/block
Running command: chown -R ceph:ceph /dev/dm-0
Running command: chown -R ceph:ceph /var/lib/ceph/osd/ceph-0
Running command: systemctl enable ceph-volume@lvm-0-f2440eea-04af-4167-944e-f1a833273412
 stderr: Created symlink from /etc/systemd/system/multi-user.target.wants/ceph-volume@lvm-0-f2440eea-04af-4167-944e-f1a833273412.service to /usr/lib/systemd/system/ceph-volume@.service.
Running command: systemctl start ceph-osd@0
--> ceph-volume lvm activate successful for osd ID: 0
--> ceph-volume lvm create successful for: /dev/sdb
```

* 由于部署mon时，ceph01节点已保存了bootstrap-osd ceph.keyring文件，因此可直接执行ceph-volume命令创建osd；
* ceph01节点将/dev/sdb配置为osd；

### Ceph02节点部署OSD

```bash
[root@ceph02 ~]# ceph auth get client.bootstrap-osd > /var/lib/ceph/bootstrap-osd/ceph.keyring
exported keyring for client.bootstrap-osd
[root@ceph02 ~]# chown ceph:ceph /var/lib/ceph/bootstrap-osd/ceph.keyring
[root@ceph02 ~]# ceph-volume lvm create --data /dev/sdb
```

* 需要获取bootstrap-osd ceph.keyring文件，并保存到相应目录，否则执行执行ceph-volume将出错；
* ceph02节点将/dev/sdb配置为osd；

### Ceph03节点部署OSD

```bash
[root@ceph03 ~]# ceph auth get client.bootstrap-osd > /var/lib/ceph/bootstrap-osd/ceph.keyring
exported keyring for client.bootstrap-osd
[root@ceph03 ~]# chown ceph:ceph /var/lib/ceph/bootstrap-osd/ceph.keyring
[root@ceph03 ~]# ceph-volume lvm create --data /dev/sdb
```

* ceph03节点将/dev/sdb配置为osd；

### 创建剩余OSD

```bash
[root@ceph01 ~]# ceph-volume lvm create --data /dev/sdc
[root@ceph02 ~]# ceph-volume lvm create --data /dev/sdc
[root@ceph03 ~]# ceph-volume lvm create --data /dev/sdc
```

* 各节点上将/dev/sdc配置为osd；

### 检查集群状态

```bash
[root@ceph01 ~]# ceph -s
  cluster:
    id:     5882486a-47fc-4178-8e41-8e8d88bf2b9d
    health: HEALTH_WARN
            no active mgr

  services:
    mon: 3 daemons, quorum ceph01,ceph02,ceph03
    mgr: no daemons active
    osd: 6 osds: 6 up, 6 in

  data:
    pools:   0 pools, 0 pgs
    objects: 0 objects, 0 bytes
    usage:   0 kB used, 0 kB / 0 kB avail
    pgs:

[root@ceph01 ~]# ceph osd tree
ID CLASS WEIGHT  TYPE NAME       STATUS REWEIGHT PRI-AFF
-1       0.05878 root default
-3       0.01959     host ceph01
 0   hdd 0.00980         osd.0       up  1.00000 1.00000
 3   hdd 0.00980         osd.3       up  1.00000 1.00000
-5       0.01959     host ceph02
 1   hdd 0.00980         osd.1       up  1.00000 1.00000
 4   hdd 0.00980         osd.4       up  1.00000 1.00000
-7       0.01959     host ceph03
 2   hdd 0.00980         osd.2       up  1.00000 1.00000
 5   hdd 0.00980         osd.5       up  1.00000 1.00000
```

* 检查集群状态和osd信息。

## 部署Filestore OSD

### Ceph01节点创建OSD

```bash
[root@ceph01 ~]# UUID=$(uuidgen)
[root@ceph01 ~]# echo $UUID
b19d57c7-de6a-415e-8fcd-75b76586b30b
```

* 创建osd uuid

```bash
[root@ceph01 ~]# OSD_SECRET=$(ceph-authtool --gen-print-key)
[root@ceph01 ~]# echo $OSD_SECRET
AQBzbx9bh15qHBAAdgea2G9dpuGhMSqZ3JRnSw==
```

* 创建cephx key；

```bash
[root@ceph01 ~]# ID=$(echo "{\"cephx_secret\": \"$OSD_SECRET\"}" | \
   ceph osd new $UUID -i - \
   -n client.bootstrap-osd -k /var/lib/ceph/bootstrap-osd/ceph.keyring)
```

* 创建osd；

```bash
[root@ceph01 ~]# mkdir /var/lib/ceph/osd/ceph-$ID
[root@ceph01 ~]# mkfs.xfs /dev/sdc
meta-data=/dev/sdc               isize=512    agcount=4, agsize=655360 blks
         =                       sectsz=512   attr=2, projid32bit=1
         =                       crc=1        finobt=0, sparse=0
data     =                       bsize=4096   blocks=2621440, imaxpct=25
         =                       sunit=0      swidth=0 blks
naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
log      =internal log           bsize=4096   blocks=2560, version=2
         =                       sectsz=512   sunit=0 blks, lazy-count=1
realtime =none                   extsz=4096   blocks=0, rtextents=0
[root@ceph01 ~]# mount /dev/sdc /var/lib/ceph/osd/ceph-$ID
[root@ceph01 ~]# vi /etc/fstab
/dev/sdc        /var/lib/ceph/osd/ceph-0        xfs     defaults        0 0
```

* 为osd准备设置默认目录；

```bash
[root@ceph01 ~]# ceph-authtool --create-keyring /var/lib/ceph/osd/ceph-$ID/keyring \
    --name osd.$ID --add-key $OSD_SECRET
creating /var/lib/ceph/osd/ceph-0/keyring
added entity osd.0 auth auth(auid = 18446744073709551615 key=AQBzbx9bh15qHBAAdgea2G9dpuGhMSqZ3JRnSw== with 0 caps)
```

* 创建osd keyring;

```bash
[root@ceph01 ~]# ceph-osd -i $ID --mkfs --osd-uuid $UUID
2018-06-12 15:10:32.489938 7f23bbf2ed80 -1 journal FileJournal::_open: disabling aio for non-block journal.  Use journal_force_aio to force use of aio anyway
2018-06-12 15:10:32.502607 7f23bbf2ed80 -1 journal FileJournal::_open: disabling aio for non-block journal.  Use journal_force_aio to force use of aio anyway
2018-06-12 15:10:32.502645 7f23bbf2ed80 -1 journal do_read_entry(8192): bad header magic
2018-06-12 15:10:32.502654 7f23bbf2ed80 -1 journal do_read_entry(8192): bad header magic
2018-06-12 15:10:32.506923 7f23bbf2ed80 -1 created object store /var/lib/ceph/osd/ceph-0 for osd.0 fsid 5882486a-47fc-4178-8e41-8e8d88bf2b9d
[root@ceph01 ~]# chown -R ceph:ceph /var/lib/ceph/osd/ceph-$ID
```

* 初始化osd数据目录；
* 初始化完成后，修改`/var/lib/ceph/osd/ceph-$ID`属主为ceph；

### 启动Ceph OSD服务

```bash
[root@ceph01 ~]# systemctl enable ceph-osd@$ID
Created symlink from /etc/systemd/system/ceph-osd.target.wants/ceph-osd@0.service to /usr/lib/systemd/system/ceph-osd@.service.
[root@ceph01 ~]# systemctl start ceph-osd@$ID
[root@ceph01 ~]# systemctl status ceph-osd@$ID
● ceph-osd@0.service - Ceph object storage daemon osd.0
   Loaded: loaded (/usr/lib/systemd/system/ceph-osd@.service; enabled; vendor preset: disabled)
   Active: active (running) since Tue 2018-06-12 15:13:57 CST; 7s ago
  Process: 2816 ExecStartPre=/usr/lib/ceph/ceph-osd-prestart.sh --cluster ${CLUSTER} --id %i (code=exited, status=0/SUCCESS)
 Main PID: 2822 (ceph-osd)
   CGroup: /system.slice/system-ceph@osd.slice/ceph-osd@0.service
           └─2822 /usr/bin/ceph-osd -f --cluster ceph --id 0 --setuser ceph --setgroup ceph

Jun 12 15:13:57 ceph01 systemd[1]: Starting Ceph object storage daemon osd.0...
Jun 12 15:13:57 ceph01 systemd[1]: Started Ceph object storage daemon osd.0.
Jun 12 15:13:57 ceph01 ceph-osd[2822]: starting osd.0 at - osd_data /var/lib/ceph/osd/ceph-0 /var/lib/ceph/osd/ceph-0/journal
Jun 12 15:13:57 ceph01 ceph-osd[2822]: 2018-06-12 15:13:57.663010 7f37e2690d80 -1 journal FileJournal::_open: disabling aio for non-block journa...io anyway
Jun 12 15:13:57 ceph01 ceph-osd[2822]: 2018-06-12 15:13:57.663418 7f37e2690d80 -1 journal do_read_entry(8192): bad header magic
Jun 12 15:13:57 ceph01 ceph-osd[2822]: 2018-06-12 15:13:57.663445 7f37e2690d80 -1 journal do_read_entry(8192): bad header magic
Jun 12 15:13:57 ceph01 ceph-osd[2822]: 2018-06-12 15:13:57.692964 7f37e2690d80 -1 osd.0 0 log_to_monitors {default=true}
Jun 12 15:13:57 ceph01 ceph-osd[2822]: 2018-06-12 15:13:57.699103 7f37c5479700 -1 osd.0 0 waiting for initial osdmap
Hint: Some lines were ellipsized, use -l to show in full.
```

* 通过systemd启动服务。

### ceph02节点增加OSD

```bash
[root@ceph02 ~]# ceph osd create
1
```

* 使用ceph osd create创建osd，不指定id，ceph将自动分配id，这里自动分配的id为1；

```bash
[root@ceph02 ~]# sudo -u ceph mkdir /var/lib/ceph/osd/ceph-1
[root@ceph02 ~]# mkfs.xfs /dev/sdc
meta-data=/dev/sdc               isize=512    agcount=4, agsize=655360 blks
         =                       sectsz=512   attr=2, projid32bit=1
         =                       crc=1        finobt=0, sparse=0
data     =                       bsize=4096   blocks=2621440, imaxpct=25
         =                       sunit=0      swidth=0 blks
naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
log      =internal log           bsize=4096   blocks=2560, version=2
         =                       sectsz=512   sunit=0 blks, lazy-count=1
realtime =none                   extsz=4096   blocks=0, rtextents=0
[root@ceph02 ~]# vi /etc/fstab
/dev/sdc        /var/lib/ceph/osd/ceph-0        xfs     defaults        0 0
[root@ceph02 ~]# mount /dev/sdc
```

* 创建osd运行目录，`ceph-1`中的`1`即为osd的id；

```bash
[root@ceph02 ~]# mkdir empty
[root@ceph02 ~]# cd empty/
[root@ceph02 empty]# ceph-osd -i 1 --mkfs --mkkey
2018-07-11 21:24:13.059999 7f6c47c71d80 -1 auth: error reading file: /var/lib/ceph/osd/ceph-1/keyring: can\'t open /var/lib/ceph/osd/ceph-1/keyring: (2) No such file or directory
2018-07-11 21:24:13.060464 7f6c47c71d80 -1 created new key in keyring /var/lib/ceph/osd/ceph-1/keyring
2018-07-11 21:24:13.077318 7f6c47c71d80 -1 journal FileJournal::_open: disabling aio for non-block journal.  Use journal_force_aio to force use of aio anyway
2018-07-11 21:24:13.091368 7f6c47c71d80 -1 journal FileJournal::_open: disabling aio for non-block journal.  Use journal_force_aio to force use of aio anyway
2018-07-11 21:24:13.091661 7f6c47c71d80 -1 journal do_read_entry(4096): bad header magic
2018-07-11 21:24:13.091749 7f6c47c71d80 -1 journal do_read_entry(4096): bad header magic
2018-07-11 21:24:13.092460 7f6c47c71d80 -1 read_settings error reading settings: (2) No such file or directory
2018-07-11 21:24:13.103156 7f6c47c71d80 -1 created object store /var/lib/ceph/osd/ceph-1 for osd.1 fsid 5882486a-47fc-4178-8e41-8e8d88bf2b9d
```

* 初始化osd运行目录，创建keyring；
* ceph-osd要在空目录中运行；

```bash
[root@ceph02 empty]# ceph auth add osd.1 osd 'allow *' mon 'allow rwx' -i /var/lib/ceph/osd/ceph-1/keyring
added key for osd.1
```

* 注册osd认证密钥；

```bash
[root@ceph02 empty]# ceph osd crush add osd.1 0.0098 root=default host=ceph02
add item id 1 name 'osd.1' weight 0.0098 at location {host=ceph02,root=default} to crush map
```

* 将osd加入crush map，由于测试的`/dev/sdc`容量性能相同，因此权重也设置为0.0098；

```bash
[root@ceph02 empty]# chown ceph:ceph -R /var/lib/ceph/osd/
```

* 由于systemd启动osd使用ceph用户和组，因此需要修改osd运行目录属主为ceph，否则无法启动osd；

```bash
[root@ceph02 empty]# systemctl enable ceph-osd@1
Created symlink from /etc/systemd/system/ceph-osd.target.wants/ceph-osd@1.service to /usr/lib/systemd/system/ceph-osd@.service.
[root@ceph02 empty]# systemctl start ceph-osd@1
[root@ceph02 empty]# systemctl status ceph-osd@1
● ceph-osd@1.service - Ceph object storage daemon osd.1
   Loaded: loaded (/usr/lib/systemd/system/ceph-osd@.service; enabled; vendor preset: disabled)
   Active: active (running) since Wed 2018-07-11 21:49:44 CST; 12s ago
 Main PID: 3067 (ceph-osd)
   CGroup: /system.slice/system-ceph\x2dosd.slice/ceph-osd@1.service
           └─3067 /usr/bin/ceph-osd -f --cluster ceph --id 1 --setuser ceph --setgroup ceph

Jul 11 21:49:44 ceph02 systemd[1]: Starting Ceph object storage daemon osd.1...
Jul 11 21:49:44 ceph02 systemd[1]: Started Ceph object storage daemon osd.1.
Jul 11 21:49:44 ceph02 ceph-osd[3067]: starting osd.1 at - osd_data /var/lib/ceph/osd/ceph-1 /var/lib/ceph/osd/ceph-1/journal
Jul 11 21:49:44 ceph02 ceph-osd[3067]: 2018-07-11 21:49:44.437731 7f183ed21d80 -1 journal FileJournal::_open: disabling aio for non-block journal.  Use jo...aio anyway
Jul 11 21:49:44 ceph02 ceph-osd[3067]: 2018-07-11 21:49:44.438269 7f183ed21d80 -1 journal do_read_entry(8192): bad header magic
Jul 11 21:49:44 ceph02 ceph-osd[3067]: 2018-07-11 21:49:44.438299 7f183ed21d80 -1 journal do_read_entry(8192): bad header magic
Jul 11 21:49:44 ceph02 ceph-osd[3067]: 2018-07-11 21:49:44.475214 7f183ed21d80 -1 osd.1 0 log_to_monitors {default=true}
Jul 11 21:49:45 ceph02 ceph-osd[3067]: 2018-07-11 21:49:45.355955 7f1821b0a700 -1 osd.1 0 waiting for initial osdmap
Hint: Some lines were ellipsized, use -l to show in full.
```

* 通过systemd启动osd实例；

```bash
[root@ceph02 empty]# ceph osd tree
ID CLASS WEIGHT  TYPE NAME       STATUS REWEIGHT PRI-AFF
-1       0.01959 root default
-3       0.00980     host ceph01
 0   hdd 0.00980         osd.0       up  1.00000 1.00000
-5       0.00980     host ceph02
 1       0.00980         osd.1       up  1.00000 1.00000
```

* 检查osd tree和ceph的状态；

### Ceph03节点增加OSD

```bash
[root@ceph03 ~]# ceph osd create
2
```

* 使用ceph osd create创建osd，不指定id，ceph将自动分配id，这里自动分配的id为2；

```bash
[root@ceph03 ~]# sudo -u ceph mkdir /var/lib/ceph/osd/ceph-2
[root@ceph03 ~]# mkfs.xfs /dev/sdc
meta-data=/dev/sdc               isize=512    agcount=4, agsize=655360 blks
         =                       sectsz=512   attr=2, projid32bit=1
         =                       crc=1        finobt=0, sparse=0
data     =                       bsize=4096   blocks=2621440, imaxpct=25
         =                       sunit=0      swidth=0 blks
naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
log      =internal log           bsize=4096   blocks=2560, version=2
         =                       sectsz=512   sunit=0 blks, lazy-count=1
realtime =none                   extsz=4096   blocks=0, rtextents=0

[root@ceph03 ~]# cat /etc/fstab
/dev/sdc        /var/lib/ceph/osd/ceph-2        xfs     defaults        0 0
[root@ceph03 ~]# mount /dev/sdc
```

* 创建osd运行目录，`ceph-2`中的`2`即为osd的id；

```bash
[root@ceph03 ~]# mkdir empty
[root@ceph03 ~]# cd empty/
[root@ceph03 empty]# ceph-osd -i 2 --mkfs --mkkey
2018-07-12 10:11:30.827397 7fcb7e05ed80 -1 auth: error reading file: /var/lib/ceph/osd/ceph-2/keyring: can\'t open /var/lib/ceph/osd/ceph-2/keyring: (2) No such file or directory
2018-07-12 10:11:30.828458 7fcb7e05ed80 -1 created new key in keyring /var/lib/ceph/osd/ceph-2/keyring
2018-07-12 10:11:30.854382 7fcb7e05ed80 -1 journal FileJournal::_open: disabling aio for non-block journal.  Use journal_force_aio to force use of aio anyway
2018-07-12 10:11:30.869117 7fcb7e05ed80 -1 journal FileJournal::_open: disabling aio for non-block journal.  Use journal_force_aio to force use of aio anyway
2018-07-12 10:11:30.869362 7fcb7e05ed80 -1 journal do_read_entry(4096): bad header magic
2018-07-12 10:11:30.869483 7fcb7e05ed80 -1 journal do_read_entry(4096): bad header magic
2018-07-12 10:11:30.870177 7fcb7e05ed80 -1 read_settings error reading settings: (2) No such file or directory
2018-07-12 10:11:30.880938 7fcb7e05ed80 -1 created object store /var/lib/ceph/osd/ceph-2 for osd.2 fsid 5882486a-47fc-4178-8e41-8e8d88bf2b9d
```

* 初始化osd运行目录，创建keyring；
* ceph-osd要在空目录中运行；

```bash
[root@ceph03 empty]# ceph auth add osd.2 osd 'allow *' mon 'allow rwx' -i /var/lib/ceph/osd/ceph-2/keyring
added key for osd.2
```

* 注册osd认证密钥；

```bash
[root@ceph03 empty]# ceph osd crush add osd.2 0.0098 root=default host=ceph03
add item id 2 name 'osd.2' weight 0.0098 at location {host=ceph03,root=default} to crush map
```

* 将osd加入crush map，由于测试的`/dev/sdc`容量性能相同，因此权重也设置为0.0098；

```bash
[root@ceph03 empty]# chown ceph:ceph -R /var/lib/ceph/osd/
```

* 由于systemd启动osd使用ceph用户和组，因此需要修改osd运行目录属主为ceph，否则无法启动osd；

```bash
[root@ceph03 empty]# systemctl enable ceph-osd@2
Created symlink from /etc/systemd/system/ceph-osd.target.wants/ceph-osd@2.service to /usr/lib/systemd/system/ceph-osd@.service.
[root@ceph03 empty]# systemctl start ceph-osd@2
[root@ceph03 empty]# systemctl status ceph-osd@2
● ceph-osd@2.service - Ceph object storage daemon osd.2
   Loaded: loaded (/usr/lib/systemd/system/ceph-osd@.service; enabled; vendor preset: disabled)
   Active: active (running) since Thu 2018-07-12 10:13:29 CST; 3s ago
  Process: 1723 ExecStartPre=/usr/lib/ceph/ceph-osd-prestart.sh --cluster ${CLUSTER} --id %i (code=exited, status=0/SUCCESS)
 Main PID: 1728 (ceph-osd)
   CGroup: /system.slice/system-ceph\x2dosd.slice/ceph-osd@2.service
           └─1728 /usr/bin/ceph-osd -f --cluster ceph --id 2 --setuser ceph --setgroup ceph

Jul 12 10:13:29 ceph03 systemd[1]: Starting Ceph object storage daemon osd.2...
Jul 12 10:13:29 ceph03 systemd[1]: Started Ceph object storage daemon osd.2.
Jul 12 10:13:29 ceph03 ceph-osd[1728]: starting osd.2 at - osd_data /var/lib/ceph/osd/ceph-2 /var/lib/ceph/osd/ceph-2/journal
Jul 12 10:13:29 ceph03 ceph-osd[1728]: 2018-07-12 10:13:29.253305 7f7b374bed80 -1 journal FileJournal::_open: disabling aio for non-block journal.  Use jo...aio anyway
Jul 12 10:13:29 ceph03 ceph-osd[1728]: 2018-07-12 10:13:29.253525 7f7b374bed80 -1 journal do_read_entry(8192): bad header magic
Jul 12 10:13:29 ceph03 ceph-osd[1728]: 2018-07-12 10:13:29.253553 7f7b374bed80 -1 journal do_read_entry(8192): bad header magic
Jul 12 10:13:29 ceph03 ceph-osd[1728]: 2018-07-12 10:13:29.284579 7f7b374bed80 -1 osd.2 0 log_to_monitors {default=true}
Jul 12 10:13:29 ceph03 ceph-osd[1728]: 2018-07-12 10:13:29.360619 7f7b1a2a7700 -1 osd.2 0 waiting for initial osdmap
Hint: Some lines were ellipsized, use -l to show in full.
```

* 通过systemd启动osd实例；

```bash
[root@ceph03 empty]# ceph osd tree
ID CLASS WEIGHT  TYPE NAME       STATUS REWEIGHT PRI-AFF
-1       0.02939 root default
-3       0.00980     host ceph01
 0   hdd 0.00980         osd.0       up  1.00000 1.00000
-5       0.00980     host ceph02
 1   hdd 0.00980         osd.1       up  1.00000 1.00000
-7       0.00980     host ceph03
 2   hdd 0.00980         osd.2       up  1.00000 1.00000
```

* 检查osd tree状态。