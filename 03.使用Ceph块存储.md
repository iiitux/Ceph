# Ceph块存储使用

## ceph rbd客户端

### 推送客户端配置

```bash
[ceph@ceph-client cluster]$ sudo ceph-deploy config push ceph-client
```

* 将ceph.conf推送到ceph-client；

### 创建rbd的客户端账户

```bash
[ceph@ceph-client cluster]$ sudo ceph auth get-or-create client.rbd mon 'allow r' osd 'allow class-read object_prefix rbd_children, allow rwx pool=rbd'
[client.rbd]
  key = AQCQmP1aIC7wNxAAC0VWE/lCb/J7T3sfWmdL/g==
[ceph@ceph-client cluster]$ sudo ceph auth get-or-create client.rbd | ssh ceph@ceph-client sudo tee /etc/ceph/ceph.client.rbd.keyring
[client.rbd]
  key = AQCQmP1aIC7wNxAAC0VWE/lCb/J7T3sfWmdL/g==
```

* 默认的client.admin权限很高，不适合分发到ceph-client节点用于访问特定存储池，因此新建用户client.rbd用户用于访问rbd存储池；
* 创建完用户后，将client.rbd密钥复制到ceph-client的/etc/ceph/ceph.client.rbd.keyring；

```bash
[ceph@ceph-client ~]$ sudo ceph -s --name client.rbd
  cluster:
    id:     870de175-7f08-4931-a96e-3b8b9ba83484
    health: HEALTH_OK

  services:
    mon: 3 daemons, quorum ceph01,ceph02,ceph03
    mgr: ceph01(active), standbys: ceph02, ceph03
    osd: 9 osds: 9 up, 9 in
    rgw: 3 daemons active

  data:
    pools:   5 pools, 288 pgs
    objects: 235 objects, 14625 kB
    usage:   10917 MB used, 81206 MB / 92124 MB avail
    pgs:     288 active+clean
```

* 在客户端，通过ceph -s并指定用户为client.rbd查看集群状态信息。

## 创建ceph块设备

```bash
[ceph@ceph-client cluster]$ sudo rbd create rbd-p0 --size 4192 --name client.rbd
```

* 由于client.rbd有rbd pool的所有权限，因此能够在client节点通过client.rbd用户，使用命令创建卷rbd-p0，大小4G；

```bash
[ceph@ceph-client cluster]$ sudo rbd ls --name client.rbd
rbd-p0
[ceph@ceph-client cluster]$ sudo rbd list --name client.rbd
rbd-p0
[ceph@ceph-client cluster]$ sudo rbd ls -p rbd --name client.rbd
rbd-p0
```

* 列出rbd镜像，不指定pool的话，默认pool为rbd。

```bash
[ceph@ceph-client ~]$ sudo rbd --image rbd-p0 info --name client.rbd
rbd image 'rbd-p0':
  size 4192 MB in 1048 objects
  order 22 (4096 kB objects)
  block_name_prefix: rbd_data.109e643c9869
  format: 2
  features: layering, exclusive-lock
  flags:
  create_timestamp: Thu May 17 23:00:41 2018
```

* 查看新创建的rbd-p0卷的详细信息。

## 映射ceph块设备

```bash
[ceph@ceph-client cluster]$ sudo rbd map --image rbd-p0 --name client.rbd
rbd: sysfs write failed
RBD image feature set mismatch. You can disable features unsupported by the kernel with "rbd feature disable rbd-p0 object-map fast-diff deep-flatten".
In some cases useful info is found in syslog - try "dmesg | tail".
rbd: map failed: (6) No such device or address

[ceph@ceph-client cluster]$ sudo grep unsupport /var/log/messages
May 16 16:59:04 CentOS kernel: rbd: image rbd-p0: image uses unsupported features: 0x38
May 16 16:59:46 CentOS kernel: rbd: image rbd-p0: image uses unsupported features: 0x38

[ceph@ceph-client cluster]$ sudo rbd feature disable rbd-p0 object-map fast-diff deep-flatten --name client.rbd

[ceph@ceph-client cluster]$ sudo rbd map --image rbd-p0 --name client.rbd
/dev/rbd0

[ceph@ceph-client cluster]$ ls -l /dev/rbd0
brw-rw---- 1 root disk 252, 0 May 16 17:21 /dev/rbd0
[ceph@ceph-client cluster]$ ls -l /dev/rbd/rbd/rbd-p0
lrwxrwxrwx 1 root root 10 May 16 17:21 /dev/rbd/rbd/rbd-p0 -> ../../rbd0

[ceph@ceph-client cluster]$ sudo fdisk -l /dev/rbd0

Disk /dev/rbd0: 4395 MB, 4395630592 bytes, 8585216 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 4194304 bytes / 4194304 bytes
```

* 通过rbd命令映射rbd-p0卷，但出现报错，当前centos的3.10内核不支持rbd的某些特性；
* 通过提示禁用rbd-p0的"object-map fast-diff deep-flatten"特性；
* 重新映射后，在client端生成/dev/rbd0块设备；

## 使用快设备

```bash
[ceph@ceph-client cluster]$ sudo mkfs.xfs /dev/rbd0
meta-data=/dev/rbd0              isize=512    agcount=8, agsize=134144 blks
         =                       sectsz=512   attr=2, projid32bit=1
         =                       crc=1        finobt=0, sparse=0
data     =                       bsize=4096   blocks=1073152, imaxpct=25
         =                       sunit=1024   swidth=1024 blks
naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
log      =internal log           bsize=4096   blocks=2560, version=2
         =                       sectsz=512   sunit=8 blks, lazy-count=1
realtime =none                   extsz=4096   blocks=0, rtextents=0

[ceph@ceph-client cluster]$ sudo mount /dev/rbd0  /mnt
[ceph@ceph-client cluster]$ sudo df -lh
Filesystem               Size  Used Avail Use% Mounted on
/dev/mapper/centos-root   20G  1.8G   18G   9% /
devtmpfs                 899M     0  899M   0% /dev
tmpfs                    911M     0  911M   0% /dev/shm
tmpfs                    911M  9.4M  902M   2% /run
tmpfs                    911M     0  911M   0% /sys/fs/cgroup
/dev/sda1                509M  136M  374M  27% /boot
tmpfs                    183M     0  183M   0% /run/user/1000
/dev/rbd0                4.1G   33M  4.1G   1% /mnt

[ceph@ceph-client cluster]$ sudo dd if=/dev/zero of=/mnt/test.iso count=1000 bs=1M
1000+0 records in
1000+0 records out
1048576000 bytes (1.0 GB) copied, 28.4126 s, 36.9 MB/s
```

* 创建xfs文件系统并挂载到/mnt，通过dd测试顺序写入，可能虚拟机原因，速度只有36.9MB/s。

## 自动映射rbd卷

```bash
[ceph@ceph-client cluster]$ sudo vi /etc/ceph/rbdmap
rbd/rbd-p0 id=rbd,keyring=/etc/ceph/ceph.client.rbd.keyring
[ceph@ceph-client cluster]$ sudo systemctl enable rbdmap.service
Created symlink from /etc/systemd/system/multi-user.target.wants/rbdmap.service to /usr/lib/systemd/system/rbdmap.service.
[ceph@ceph-client cluster]$ sudo systemctl start rbdmap.service
```

* CentOS为rbd提供了rbdmap.service systemd service unit，通过修改/etc/ceph/rbdmap配置文件可实现自动映射rbd卷；
* rbdmap的配置中的id，无需"client."。

## 调整Ceph RBD大小

```bash
[ceph@ceph-client ~]$ sudo rbd resize --image rbd-p0 --size 5120 --name client.rbd
Resizing image: 100% complete...done.
[ceph@ceph-client ~]$ sudo rbd info --image rbd-p0 --name client.rbd
rbd image 'rbd-p0':
  size 5120 MB in 1280 objects
  order 22 (4096 kB objects)
  block_name_prefix: rbd_data.fab3643c9869
  format: 2
  features: layering, exclusive-lock
  flags:
  create_timestamp: Sun Jun  3 21:25:12 2018
[ceph@ceph-client ~]$ sudo dmesg |grep -i capacity
[   20.799150] rbd: rbd0: capacity 2147483648 features 0x1
[  209.171227] rbd: rbd0: capacity 4294967296 features 0x5
[ 1146.324944] rbd0: detected capacity change from 4294967296 to 5368709120
```

## 使用RBD快照

```bash
[ceph@ceph-client ~]$ cd /mnt
[ceph@ceph-client mnt]$ ls
[ceph@ceph-client mnt]$ echo "test" |sudo tee test
test
[ceph@ceph-client mnt]$ cat test
test
[ceph@ceph-client mnt]$ sudo sync
[ceph@ceph-client mnt]$ sudo rbd snap create rbd/rbd-p0@snapshot01 --name client.rbd
[ceph@ceph-client mnt]$ sudo rbd snap ls rbd/rbd-p0 --name client.rbd
SNAPID NAME          SIZE TIMESTAMP
    10 snapshot01 5120 MB Sun Jun  3 21:49:04 2018
```

* 测试中使用sync命令将buffer刷新到文件，确保后续能正常读出文件内容；

```bash
[ceph@ceph-client mnt]$ sudo rm test
[ceph@ceph-client mnt]$ sudo rbd snap rollback rbd/rbd-p0@snapshot01 --name client.rbd
Rolling back to snapshot: 0% complete...failed.
rbd: rollback failed: (30) Read-only file system
[ceph@ceph-client mnt]$ sudo rbd feature disable rbd-p0 exclusive-lock --name client.rbd
[ceph@ceph-client mnt]$ cd
[ceph@ceph-client ~]$ sudo umount /mnt
[ceph@ceph-client mnt]$ sudo rbd snap rollback rbd/rbd-p0@snapshot01 --name client.rbd
Rolling back to snapshot: 100% complete...done.
[ceph@ceph-client ~]$ sudo mount /dev/rbd0 /mnt
[ceph@ceph-client ~]$ cd /mnt/
[ceph@ceph-client mnt]$ ls
test
[ceph@ceph-client mnt]$ cat test
test
```

* exclusive-lock会导致回滚快照失败，关闭后正常；
* rollback前先卸载文件系统，rollback后再重新挂载确认文件内容；

## 使用rbd克隆

```bash
[ceph@ceph-client ~]$ sudo rbd snap create rbd/rbd-p0@snap_for_cloning --name client.rbd
[ceph@ceph-client ~]$ sudo rbd snap protect rbd/rbd-p0@snap_for_cloning --name client.rbd
[ceph@ceph-client ~]$ sudo rbd info --image rbd-p0@snap_for_cloning --name client.rbd
rbd image 'rbd-p0':
  size 5120 MB in 1280 objects
  order 22 (4096 kB objects)
  block_name_prefix: rbd_data.fab3643c9869
  format: 2
  features: layering
  flags:
  create_timestamp: Sun Jun  3 21:25:12 2018
  protected: True
[ceph@ceph-client ~]$ sudo rbd clone rbd/rbd-p0@snap_for_cloning rbd/rbd-cloning01 --name client.rbd
[ceph@ceph-client ~]$ sudo rbd info rbd/rbd-cloning01
rbd image 'rbd-cloning01':
  size 5120 MB in 1280 objects
  order 22 (4096 kB objects)
  block_name_prefix: rbd_data.fb58238e1f29
  format: 2
  features: layering
  flags:
  create_timestamp: Mon Jun  4 10:17:05 2018
  parent: rbd/rbd-p0@snap_for_cloning
  overlap: 5120 MB
```

* 克隆前需要设置快照保护，避免父镜像删除导致子镜像一并删除，设置保护后查看快照信息：`protected: True`；
* 克隆后，查看rbd信息：`parent: rbd/rbd-p0@snap_for_cloning`

```bash
[ceph@ceph-client ~]$ sudo rbd flatten rbd/rbd-cloning01 --name client.rbd
Image flatten: 100% complete...done.
[ceph@ceph-client ~]$ sudo rbd info rbd-cloning01
rbd image 'rbd-cloning01':
  size 5120 MB in 1280 objects
  order 22 (4096 kB objects)
  block_name_prefix: rbd_data.fb58238e1f29
  format: 2
  features: layering
  flags:
  create_timestamp: Mon Jun  4 10:17:05 2018
```

* 克隆完成后，rbd-cloning01依赖于快照rbd/rbd-p0@snap_for_cloning，可通过rbd flatten将父镜像信息合并到子镜像rbd-cloning01，通过rbd info查看，`parent: rbd/rbd-p0@snap_for_cloning`消失。

```bash
[ceph@ceph-client ~]$ sudo rbd snap unprotect rbd/rbd-p0@snap_for_cloning --name client.rbd
[ceph@ceph-client ~]$ sudo rbd info rbd/rbd-p0@snap_for_cloning --name client.rbd
rbd image 'rbd-p0':
  size 5120 MB in 1280 objects
  order 22 (4096 kB objects)
  block_name_prefix: rbd_data.fab3643c9869
  format: 2
  features: layering
  flags:
  create_timestamp: Sun Jun  3 21:25:12 2018
  protected: False
[ceph@ceph-client ~]$ sudo rbd snap remove rbd/rbd-p0@snap_for_cloning --name client.rbd
Removing snap: 100% complete...done.
[ceph@ceph-client ~]$ sudo rbd snap ls
rbd: image name was not specified
```

* 当父镜像快照不再使用时，可先取消保护，再删除该快照。