# Cephfs使用

## 创建cephfs

```bash
[root@ceph01 ~]# ceph osd pool create cephfs_data 32 32
pool 'cephfs_data' created
[root@ceph01 ~]# ceph osd pool create cephfs_metadata 32 32
pool 'cephfs_metadata' created
[root@ceph01 ~]# ceph fs new cephfs cephfs_data cephfs_metadata
new fs with metadata pool 6 and data pool 7
[root@ceph01 ~]# ceph fs ls
name: cephfs, metadata pool: cephfs_data, data pools: [cephfs_metadata ]
```

* cephfs需要一个数据池、一个metadata池，数据池用于存放文件的数据，而metadata池用于存放文件的inode信息；

## 挂载cephfs

### 内核方式挂载

```bash
[root@ceph01 ~]# mkdir /ceph
[root@ceph01 ~]# ceph auth get-key client.admin
AQDWuGtbpjYBAhAABmvTMDQPwYKq7JHIcpI+5w==
[root@ceph01 ~]# ceph auth get-key client.admin > keyring
[root@ceph01 ~]# chmod 600 keyring
[root@ceph01 ~]# mount -t ceph ceph01:6789:/ /ceph -o name=admin,secretfile=keyring
[root@ceph01 ~]# df -hT
Filesystem           Type      Size  Used Avail Use% Mounted on
/dev/sda2            xfs        20G  1.9G   18G  10% /
devtmpfs             devtmpfs  3.9G     0  3.9G   0% /dev
tmpfs                tmpfs     3.9G     0  3.9G   0% /dev/shm
tmpfs                tmpfs     3.9G  8.8M  3.9G   1% /run
tmpfs                tmpfs     3.9G     0  3.9G   0% /sys/fs/cgroup
/dev/sda1            xfs       509M  165M  344M  33% /boot
tmpfs                tmpfs     3.9G   24K  3.9G   1% /var/lib/ceph/osd/ceph-3
tmpfs                tmpfs     3.9G   24K  3.9G   1% /var/lib/ceph/osd/ceph-0
tmpfs                tmpfs     783M     0  783M   0% /run/user/0
172.16.16.240:6789:/ ceph       34G     0   34G   0% /ceph
[root@ceph01 ~]# mount |grep 'type ceph'
172.16.16.240:6789:/ on /ceph type ceph (rw,relatime,name=admin,secret=<hidden>,acl,wsize=16777216)
[root@ceph01 ~]# cp /var/log/messages /ceph
[root@ceph01 ~]# tail -n 5 /ceph/messages
Aug 27 23:01:01 ceph01 systemd: Starting Session 450 of user root.
Aug 27 23:01:52 ceph01 kernel: libceph: mon0 172.16.16.240:6789 session established
Aug 27 23:01:52 ceph01 kernel: libceph: client14265 fsid 7922fd83-2140-44d6-9729-9fd4a8541469
Aug 27 23:27:44 ceph01 kernel: libceph: mon0 172.16.16.240:6789 session established
Aug 27 23:27:44 ceph01 kernel: libceph: client14280 fsid 7922fd83-2140-44d6-9729-9fd4a8541469
[root@ceph01 ~]# umount /ceph
[root@ceph01 ~]# vi /etc/fstab
ceph01:6789:/     /ceph ceph        name=admin,secretfile=/root/keyring,_netdev,noatime    0   0
[root@ceph01 ~]# mount /ceph
```

* 获取保存用户的key;
* 挂载时通过`-t ceph`参数指定文件系统类型；
* `-o name=admin,secretfile=keyring`指定挂载的ceph用户和key；
* 修改`/etc/fstab`配置ceph挂载的相关参数，使得可以开机挂载cephfs；

### fuse方式挂载

```bash
[root@ceph01 ~]# yum install ceph-fuse
[root@ceph01 ~]# mkdir /ceph-fuse
[root@ceph01 ~]# ceph-fuse -m ceph01:6789 /ceph-fuse
2018-08-27 23:41:05.592598 7f647c0860c0 -1 init, newargv = 0x565008bf6c60 newargc=9ceph-fuse[
30602]: starting ceph client
ceph-fuse[30602]: starting fuse
[root@ceph01 ~]# df -hT
Filesystem           Type            Size  Used Avail Use% Mounted on
/dev/sda2            xfs              20G  1.9G   18G  10% /
devtmpfs             devtmpfs        3.9G     0  3.9G   0% /dev
tmpfs                tmpfs           3.9G     0  3.9G   0% /dev/shm
tmpfs                tmpfs           3.9G  8.8M  3.9G   1% /run
tmpfs                tmpfs           3.9G     0  3.9G   0% /sys/fs/cgroup
/dev/sda1            xfs             509M  165M  344M  33% /boot
tmpfs                tmpfs           3.9G   24K  3.9G   1% /var/lib/ceph/osd/ceph-3
tmpfs                tmpfs           3.9G   24K  3.9G   1% /var/lib/ceph/osd/ceph-0
tmpfs                tmpfs           783M     0  783M   0% /run/user/0
172.16.16.240:6789:/ ceph             34G     0   34G   0% /ceph
ceph-fuse            fuse.ceph-fuse   34G     0   34G   0% /ceph-fuse
[root@ceph01 ~]# mount |grep ceph
tmpfs on /var/lib/ceph/osd/ceph-3 type tmpfs (rw,relatime)
tmpfs on /var/lib/ceph/osd/ceph-0 type tmpfs (rw,relatime)
172.16.16.240:6789:/ on /ceph type ceph (rw,relatime,name=admin,secret=<hidden>,acl,wsize=16777216)
ceph-fuse on /ceph-fuse type fuse.ceph-fuse (rw,nosuid,nodev,relatime,user_id=0,group_id=0,allow_other)
[root@ceph01 ~]# tail -n 5 /ceph-fuse/messages
Aug 27 23:01:01 ceph01 systemd: Starting Session 450 of user root.
Aug 27 23:01:52 ceph01 kernel: libceph: mon0 172.16.16.240:6789 session established
Aug 27 23:01:52 ceph01 kernel: libceph: client14265 fsid 7922fd83-2140-44d6-9729-9fd4a8541469
Aug 27 23:27:44 ceph01 kernel: libceph: mon0 172.16.16.240:6789 session established
Aug 27 23:27:44 ceph01 kernel: libceph: client14280 fsid 7922fd83-2140-44d6-9729-9fd4a8541469
[root@ceph01 ~]# umount /ceph-fuse/
[root@ceph01 ~]# vi /etc/fstab
none      /ceph-fuse   fuse.ceph   ceph.id=admin,_netdev,defaults  0 0
[root@ceph01 ~]# mount /ceph-fuse
2018-08-28 00:09:31.326995 7fcc8d20a0c0 -1 init, newargv = 0x5619e737bb90 newargc=11ceph-fuse[30830]: starting ceph client
ceph-fuse[30830]: starting fuse
```

* 通过fuse方式挂载前要安装`ceph-fuse`软件包；
* 通过`df -hT`查看信息时，可以看到`/ceph-fuse`的文件系统类型为`fuse.ceph-fuse`;
* cephfs是一个分布式文件系统，支持对节点的并发读写操作，因此虽然挂载使用的驱动不同，但文件系统中都能看到放入的系统`messages`测试文件内容；
* ceph-fuse模式运行在用户态，操作IO时会导致频繁的用户态和内核态转换，因此效率相对内核模式直接挂载来的低。建议内核支持cephfs时优先使用内核模式挂载。