# Crush Map管理

## 查看CRUSH结构

```bash
[root@ceph01 ~]# ceph osd crush tree
ID CLASS WEIGHT  TYPE NAME
-1       0.05878 root default
-3       0.01959     host ceph01
 0   hdd 0.00980         osd.0
 3   hdd 0.00980         osd.3
-5       0.01959     host ceph02
 1   hdd 0.00980         osd.1
 4   hdd 0.00980         osd.4
-7       0.01959     host ceph03
 2   hdd 0.00980         osd.2
 5   hdd 0.00980         osd.5
```

* ceph用bucket来组织CRUSH层级结构，常见的bucket type有`osd (or device)、host、chassis、rack、row、pdu、pod、room、datacenter、region、root`；

## CRUSH规则

```bash
[root@ceph01 ~]# ceph osd crush rule ls
replicated_rule
erasure-code
```

* 查看CRUSH规则列表；

``` bash
[root@ceph01 ~]# ceph osd crush rule dump
[
    {
        "rule_id": 0,
        "rule_name": "replicated_rule",
        "ruleset": 0,
        "type": 1,
        "min_size": 1,
        "max_size": 10,
        "steps": [
            {
                "op": "take",
                "item": -1,
                "item_name": "default"
            },
            {
                "op": "chooseleaf_firstn",
                "num": 0,
                "type": "host"
            },
            {
                "op": "emit"
            }
        ]
    },
    {
        "rule_id": 1,
        "rule_name": "erasure-code",
        "ruleset": 1,
        "type": 3,
        "min_size": 3,
        "max_size": 3,
        "steps": [
            {
                "op": "set_chooseleaf_tries",
                "num": 5
            },
            {
                "op": "set_choose_tries",
                "num": 100
            },
            {
                "op": "take",
                "item": -1,
                "item_name": "default"
            },
            {
                "op": "chooseleaf_indep",
                "num": 0,
                "type": "host"
            },
            {
                "op": "emit"
            }
        ]
    }
]
```

* 查看CRUSH规则内容；

## Device-Classes

``` bash
[root@ceph01 ~]# ceph osd tree
ID CLASS WEIGHT  TYPE NAME       STATUS REWEIGHT PRI-AFF
-1       0.05878 root default
-3       0.01959     host ceph01
 3   hdd 0.00980         osd.3       up  1.00000 1.00000
 0   hdd 0.00980         osd.0       up  1.00000 1.00000
-5       0.01959     host ceph02
 1   hdd 0.00980         osd.1       up  1.00000 1.00000
 4   hdd 0.00980         osd.4       up  1.00000 1.00000
-7       0.01959     host ceph03
 2   hdd 0.00980         osd.2       up  1.00000 1.00000
 5   hdd 0.00980         osd.5       up  1.00000 1.00000
[root@ceph01 ~]# ceph osd crush rm-device-class osd.0
done removing class of osd(s): 0
[root@ceph01 ~]# ceph osd crush set-device-class ssd osd.0
set osd(s) 0 to class 'ssd'
[root@ceph01 ~]# ceph osd crush rm-device-class osd.1
done removing class of osd(s): 1
[root@ceph01 ~]# ceph osd crush set-device-class ssd osd.1
set osd(s) 1 to class 'ssd'
[root@ceph01 ~]# ceph osd crush rm-device-class osd.2
done removing class of osd(s): 2
[root@ceph01 ~]# ceph osd crush set-device-class ssd osd.2
set osd(s) 2 to class 'ssd'
[root@ceph01 ~]# ceph osd tree
ID CLASS WEIGHT  TYPE NAME       STATUS REWEIGHT PRI-AFF
-1       0.05878 root default
-3       0.01959     host ceph01
 3   hdd 0.00980         osd.3       up  1.00000 1.00000
 0   ssd 0.00980         osd.0       up  1.00000 1.00000
-5       0.01959     host ceph02
 4   hdd 0.00980         osd.4       up  1.00000 1.00000
 1   ssd 0.00980         osd.1       up  1.00000 1.00000
-7       0.01959     host ceph03
 5   hdd 0.00980         osd.5       up  1.00000 1.00000
 2   ssd 0.00980         osd.2       up  1.00000 1.00000
```

* 每个设备都可以设置一个device-class，device-class可以是hdd、ssd、nvme；
* 设置新的device-class之前，需要删除旧的device-class；

```bash
[root@ceph01 ~]# ceph osd crush rule create-replicated test-rule default host ssd
[root@ceph01 ~]# ceph osd crush rule dump test-rule
{
    "rule_id": 2,
    "rule_name": "test-rule",
    "ruleset": 2,
    "type": 1,
    "min_size": 1,
    "max_size": 10,
    "steps": [
        {
            "op": "take",
            "item": -12,
            "item_name": "default~ssd"
        },
        {
            "op": "chooseleaf_firstn",
            "num": 0,
            "type": "host"
        },
        {
            "op": "emit"
        }
    ]
}
```

* 新建test-rule，设置device-class为ssd；

```bash
[root@ceph01 ~]# ceph osd pool set rbd crush_rule test-rule
set pool 1 crush_rule to test-rule
[root@ceph01 ~]# ceph osd pool get rbd crush_rule
crush_rule: test-rule
```

* 将新建的规则应用的rbd池；

```bash
[root@ceph01 ~]# ceph osd crush tree --show-shadow
ID  CLASS WEIGHT  TYPE NAME
-12   ssd 0.00980 root default~ssd
 -9   ssd 0.00980     host ceph01~ssd
  0   ssd 0.00980         osd.0
-10   ssd       0     host ceph02~ssd
-11   ssd       0     host ceph03~ssd
 -2   hdd 0.04898 root default~hdd
 -4   hdd 0.00980     host ceph01~hdd
  3   hdd 0.00980         osd.3
 -6   hdd 0.01959     host ceph02~hdd
  1   hdd 0.00980         osd.1
  4   hdd 0.00980         osd.4
 -8   hdd 0.01959     host ceph03~hdd
  2   hdd 0.00980         osd.2
  5   hdd 0.00980         osd.5
 -1       0.05878 root default
 -3       0.01959     host ceph01
  3   hdd 0.00980         osd.3
  0   ssd 0.00980         osd.0
 -5       0.01959     host ceph02
  1   hdd 0.00980         osd.1
  4   hdd 0.00980         osd.4
 -7       0.01959     host ceph03
  2   hdd 0.00980         osd.2
  5   hdd 0.00980         osd.5
```

* 查看CRUSH的shadow结构；

## WEIGHT

```bash
[root@ceph01 ~]# ceph osd crush tree
ID CLASS WEIGHT  TYPE NAME
-1       0.05878 root default
-3       0.01959     host ceph01
 3   hdd 0.00980         osd.3
 0   ssd 0.00980         osd.0
-5       0.01959     host ceph02
 4   hdd 0.00980         osd.4
 1   ssd 0.00980         osd.1
-7       0.01959     host ceph03
 5   hdd 0.00980         osd.5
 2   ssd 0.00980         osd.2
```

* 查看CRUSH结构中的weight信息；

## 修改CRUSH MAP

### 为OSD设置BUCKET

```bash
[root@ceph01 ~]# ceph osd crush tree
ID CLASS WEIGHT  TYPE NAME
-1       0.05878 root default
-3       0.01959     host ceph01
 3   hdd 0.00980         osd.3
 0   ssd 0.00980         osd.0
-5       0.01959     host ceph02
 4   hdd 0.00980         osd.4
 1   ssd 0.00980         osd.1
-7       0.01959     host ceph03
 5   hdd 0.00980         osd.5
 2   ssd 0.00980         osd.2

[root@ceph01 ~]# ceph osd crush set osd.0 0.098 root=temp row=a rack=0
set item id 0 name 'osd.0' weight 0.098 at location {rack=0,root=temp,row=a} to crush map
[root@ceph01 ~]# ceph osd crush set osd.3 0.098 root=temp row=a rack=0
set item id 3 name 'osd.3' weight 0.098 at location {rack=0,root=temp,row=a} to crush map

[root@ceph01 ~]# ceph osd tree
ID  CLASS WEIGHT  TYPE NAME         STATUS REWEIGHT PRI-AFF
-15       0.19598 root temp
-14       0.19598     row a
-13       0.19598         rack 0
  3   hdd 0.09799             osd.3     up  1.00000 1.00000
  0   ssd 0.09799             osd.0     up  1.00000 1.00000
 -1       0.03918 root default
 -3             0     host ceph01
 -5       0.01959     host ceph02
  4   hdd 0.00980         osd.4         up  1.00000 1.00000
  1   ssd 0.00980         osd.1         up  1.00000 1.00000
 -7       0.01959     host ceph03
  5   hdd 0.00980         osd.5         up  1.00000 1.00000
  2   ssd 0.00980         osd.2         up  1.00000 1.00000

[root@ceph01 ~]# ceph osd crush set osd.0 0.00980 root=root host=ceph01
set item id 0 name 'osd.0' weight 0.0098 at location {host=ceph01,root=root} to crush map
[root@ceph01 ~]# ceph osd crush set osd.3 0.00980 root=root host=ceph01
set item id 3 name 'osd.3' weight 0.0098 at location {host=ceph01,root=root} to crush map  
[root@ceph01 ~]# ceph osd tree
ID  CLASS WEIGHT  TYPE NAME       STATUS REWEIGHT PRI-AFF
-15             0 root temp
-14             0     row a
-13             0         rack 0
 -1       0.05878 root default
 -3       0.01959     host ceph01
  3   hdd 0.00980         osd.3       up  1.00000 1.00000
  0   ssd 0.00980         osd.0       up  1.00000 1.00000
 -5       0.01959     host ceph02
  4   hdd 0.00980         osd.4       up  1.00000 1.00000
  1   ssd 0.00980         osd.1       up  1.00000 1.00000
 -7       0.01959     host ceph03
  5   hdd 0.00980         osd.5       up  1.00000 1.00000
  2   ssd 0.00980         osd.2       up  1.00000 1.00000

```

* 将某个osd加入或为osd设置bucket；
* 0.0098为osd的weight，root和host为bucket；

### 修改OSD Weight

```bash
[root@ceph01 ~]# ceph osd crush reweight osd.0 0.1
reweighted item id 0 name 'osd.0' to 0.1 in crush map
[root@ceph01 ~]# ceph osd tree
ID CLASS WEIGHT  TYPE NAME       STATUS REWEIGHT PRI-AFF
-1       0.14897 root default
-3       0.10979     host ceph01
 3   hdd 0.00980         osd.3       up  1.00000 1.00000
 0   ssd 0.09999         osd.0       up  1.00000 1.00000
-5       0.01959     host ceph02
 4   hdd 0.00980         osd.4       up  1.00000 1.00000
 1   ssd 0.00980         osd.1       up  1.00000 1.00000
-7       0.01959     host ceph03
 5   hdd 0.00980         osd.5       up  1.00000 1.00000
 2   ssd 0.00980         osd.2       up  1.00000 1.00000
```

* 在运行的集群中修改osd的CRUSH weight；

### 从CRUSH删除OSD

```bash
[root@ceph01 ~]# ceph osd crush remove osd.0
removed item id 0 name 'osd.0' from crush map
[root@ceph01 ~]# ceph osd tree
ID CLASS WEIGHT  TYPE NAME       STATUS REWEIGHT PRI-AFF
-1       0.04898 root default
-3       0.00980     host ceph01
 3   hdd 0.00980         osd.3       up  1.00000 1.00000
-5       0.01959     host ceph02
 4   hdd 0.00980         osd.4       up  1.00000 1.00000
 1   ssd 0.00980         osd.1       up  1.00000 1.00000
-7       0.01959     host ceph03
 5   hdd 0.00980         osd.5       up  1.00000 1.00000
 2   ssd 0.00980         osd.2       up  1.00000 1.00000
 0             0 osd.0               up  1.00000 1.00000

[root@ceph01 ~]# ceph osd crush add osd.0 0.0098 root=root host=ceph01
add item id 0 name 'osd.0' weight 0.0098 at location {host=ceph01,root=root} to crush map
[root@ceph01 ~]# ceph osd tree
ID CLASS WEIGHT  TYPE NAME       STATUS REWEIGHT PRI-AFF
-1       0.05878 root default
-3       0.01959     host ceph01
 0       0.00980         osd.0       up  1.00000 1.00000
 3   hdd 0.00980         osd.3       up  1.00000 1.00000
-5       0.01959     host ceph02
 4   hdd 0.00980         osd.4       up  1.00000 1.00000
 1   ssd 0.00980         osd.1       up  1.00000 1.00000
-7       0.01959     host ceph03
 5   hdd 0.00980         osd.5       up  1.00000 1.00000
 2   ssd 0.00980         osd.2       up  1.00000 1.00000
```

* 在运行着的集群中，将osd从CRUSH中移除，并重新加入到CRUSH；
* 0.0098为osd的weight，root和host为bucket；

### 增加BUCKET

```bash
[root@ceph01 ~]# ceph osd crush add-bucket a01 rack
added bucket a01 type rack to crush map
[root@ceph01 ~]# ceph osd crush tree
ID  CLASS WEIGHT  TYPE NAME
-13             0 rack a01
 -1       0.05878 root default
 -3       0.01959     host ceph01
  0       0.00980         osd.0
  3   hdd 0.00980         osd.3
 -5       0.01959     host ceph02
  4   hdd 0.00980         osd.4
  1   ssd 0.00980         osd.1
 -7       0.01959     host ceph03
  5   hdd 0.00980         osd.5
  2   ssd 0.00980         osd.2
```

* a01为bucket name，rack为bucket type；

### 移动BUCKET

```bash
[root@ceph01 ~]# ceph osd crush move a01 root=default
moved item id -13 name 'a01' to location {root=default} in crush map
[root@ceph01 ~]# ceph osd crush tree
ID  CLASS WEIGHT  TYPE NAME
 -1       0.05878 root default
-13             0     rack a01
 -3       0.01959     host ceph01
  0       0.00980         osd.0
  3   hdd 0.00980         osd.3
 -5       0.01959     host ceph02
  4   hdd 0.00980         osd.4
  1   ssd 0.00980         osd.1
 -7       0.01959     host ceph03
  5   hdd 0.00980         osd.5
  2   ssd 0.00980         osd.2
[root@ceph01 ~]# ceph osd crush add-bucket a02 rack
added bucket a02 type rack to crush map
[root@ceph01 ~]# ceph osd crush add-bucket a03 rack
added bucket a03 type rack to crush map
[root@ceph01 ~]# ceph osd crush move a02 root=default
moved item id -16 name 'a02' to location {root=default} in crush map
[root@ceph01 ~]# ceph osd crush move a03 root=default
moved item id -17 name 'a03' to location {root=default} in crush map
[root@ceph01 ~]# ceph osd crush tree
ID  CLASS WEIGHT  TYPE NAME
 -1       0.05878 root default
-13             0     rack a01
-16             0     rack a02
-17             0     rack a03
 -3       0.01959     host ceph01
  0       0.00980         osd.0
  3   hdd 0.00980         osd.3
 -5       0.01959     host ceph02
  4   hdd 0.00980         osd.4
  1   ssd 0.00980         osd.1
 -7       0.01959     host ceph03
  5   hdd 0.00980         osd.5
  2   ssd 0.00980         osd.2

[root@ceph01 ~]# ceph osd crush move ceph01 rack=a01
moved item id -3 name 'ceph01' to location {rack=a01} in crush map
[root@ceph01 ~]# ceph osd crush move ceph02 rack=a02
moved item id -5 name 'ceph02' to location {rack=a02} in crush map
[root@ceph01 ~]# ceph osd crush move ceph03 rack=a03
moved item id -7 name 'ceph03' to location {rack=a03} in crush map
[root@ceph01 ~]# ceph osd crush tree
ID  CLASS WEIGHT  TYPE NAME
 -1       0.05878 root default
-13       0.01959     rack a01
 -3       0.01959         host ceph01
  0       0.00980             osd.0
  3   hdd 0.00980             osd.3
-16       0.01959     rack a02
 -5       0.01959         host ceph02
  4   hdd 0.00980             osd.4
  1   ssd 0.00980             osd.1
-17       0.01959     rack a03
 -7       0.01959         host ceph03
  5   hdd 0.00980             osd.5
  2   ssd 0.00980             osd.2
```

* 将指定的bucket移动到其他CRUSH层级；

### 删除BUCKET

```bash
[root@ceph01 ~]# ceph osd crush add-bucket test rack
added bucket test type rack to crush map
[root@ceph01 ~]# ceph osd crush tree
ID  CLASS WEIGHT  TYPE NAME
-22             0 rack test
 -1       0.05878 root default
-13       0.01959     rack a01
 -3       0.01959         host ceph01
  0       0.00980             osd.0
  3   hdd 0.00980             osd.3
-16       0.01959     rack a02
 -5       0.01959         host ceph02
  4   hdd 0.00980             osd.4  
  1   ssd 0.00980             osd.1
-17       0.01959     rack a03
 -7       0.01959         host ceph03
  5   hdd 0.00980             osd.5
  2   ssd 0.00980             osd.2
[root@ceph01 ~]# ceph osd crush remove test
removed item id -22 name 'test' from crush map
[root@ceph01 ~]# ceph osd crush tree
ID  CLASS WEIGHT  TYPE NAME
 -1       0.05878 root default
-13       0.01959     rack a01
 -3       0.01959         host ceph01
  0       0.00980             osd.0
  3   hdd 0.00980             osd.3
-16       0.01959     rack a02
 -5       0.01959         host ceph02
  4   hdd 0.00980             osd.4
  1   ssd 0.00980             osd.1
-17       0.01959     rack a03
 -7       0.01959         host ceph03
  5   hdd 0.00980             osd.5
  2   ssd 0.00980             osd.2
```

* 删除Bucket，删除时该Bucket必须为空；

### Compat weight-set

```bash
[root@ceph01 ~]# ceph osd crush weight-set create-compat
[root@ceph01 ~]# ceph osd crush weight-set ls
(compat)
[root@ceph01 ~]# ceph osd crush tree
ID  CLASS WEIGHT  (compat) TYPE NAME
 -1       0.05878          root default
-13       0.01959  0.01959     rack a01
 -3       0.01959  0.01959         host ceph01
  0       0.00980  0.00980             osd.0
  3   hdd 0.00980  0.00980             osd.3
-16       0.01959  0.01959     rack a02
 -5       0.01959  0.01959         host ceph02
  4   hdd 0.00980  0.00980             osd.4
  1   ssd 0.00980  0.00980             osd.1
-17       0.01959  0.01959     rack a03
 -7       0.01959  0.01959         host ceph03
  5   hdd 0.00980  0.00980             osd.5
  2   ssd 0.00980  0.00980             osd.2
```

* compat weight-set可以为集群中的节点和osd设置指定的权重，不适合处理所有问题，例如不同pool的pg可能有不同的大小和处理能力，但在compat weight-set下可能被一视同仁；
* 但compat weight-set向后兼容，老客户端仍然能连接上集群使用compat weight-set来平衡数据；

```bash
[root@ceph01 ~]# ceph osd crush weight-set reweight-compat ceph01 0.02
[root@ceph01 ~]# ceph osd crush tree
ID  CLASS WEIGHT  (compat) TYPE NAME
 -1       0.05878          root default
-13       0.01959  0.01999     rack a01
 -3       0.01959  0.01999         host ceph01
  0       0.00980  0.00980             osd.0
  3   hdd 0.00980  0.00980             osd.3
-16       0.01959  0.01959     rack a02
 -5       0.01959  0.01959         host ceph02
  4   hdd 0.00980  0.00980             osd.4
  1   ssd 0.00980  0.00980             osd.1
-17       0.01959  0.01959     rack a03
 -7       0.01959  0.01959         host ceph03
  5   hdd 0.00980  0.00980             osd.5
  2   ssd 0.00980  0.00980             osd.2
```

* 将ceph01权重设置为0.02；

```bash
[root@ceph01 ~]# ceph osd crush weight-set rm-compat
[root@ceph01 ~]# ceph osd crush tree
ID  CLASS WEIGHT  TYPE NAME
 -1       0.05878 root default
-13       0.01959     rack a01
 -3       0.01959         host ceph01
  0       0.00980             osd.0
  3   hdd 0.00980             osd.3
-16       0.01959     rack a02
 -5       0.01959         host ceph02
  4   hdd 0.00980             osd.4
  1   ssd 0.00980             osd.1
-17       0.01959     rack a03
 -7       0.01959         host ceph03
  5   hdd 0.00980             osd.5
  2   ssd 0.00980             osd.2
```

* 删除compat weight-set；

### per-pool weight-set

```bash
[root@ceph01 ~]# ceph osd set-require-min-compat-client luminous
set require_min_compat_client to luminous
[root@ceph01 ~]# ceph osd crush weight-set create rbd flat
[root@ceph01 ~]# ceph osd crush weight-set ls
rbd
[root@ceph01 ~]# ceph osd crush tree
ID  CLASS WEIGHT  rbd     TYPE NAME
 -1       0.05878         root default
-13       0.01959 0.01959     rack a01
 -3       0.01959 0.01959         host ceph01
  0       0.00980 0.00980             osd.0
  3   hdd 0.00980 0.00980             osd.3
-16       0.01959 0.01959     rack a02
 -5       0.01959 0.01959         host ceph02
  4   hdd 0.00980 0.00980             osd.4
  1   ssd 0.00980 0.00980             osd.1
-17       0.01959 0.01959     rack a03
 -7       0.01959 0.01959         host ceph03
  5   hdd 0.00980 0.00980             osd.5
  2   ssd 0.00980 0.00980             osd.2
```

* 为rbd池创建per-pool weight-set，模式为`flat`或`positional`；

```bash
[root@ceph01 ~]# ceph osd crush weight-set rm rbd
[root@ceph01 ~]# ceph osd crush tree
ID  CLASS WEIGHT  TYPE NAME
 -1       0.05878 root default
-13       0.01959     rack a01
 -3       0.01959         host ceph01
  0       0.00980             osd.0
  3   hdd 0.00980             osd.3
-16       0.01959     rack a02
 -5       0.01959         host ceph02
  4   hdd 0.00980             osd.4
  1   ssd 0.00980             osd.1
-17       0.01959     rack a03
 -7       0.01959         host ceph03
  5   hdd 0.00980             osd.5
  2   ssd 0.00980             osd.2
```

* 删除rbd per-pool weight-set；

## 手动编辑CRUSH MAP

```bash
[root@ceph01 ~]# ceph osd crush rule dump test-rule
{
    "rule_id": 2,
    "rule_name": "test-rule",
    "ruleset": 2,
    "type": 1,
    "min_size": 1,
    "max_size": 10,
    "steps": [
        {
            "op": "take",
            "item": -12,
            "item_name": "default~ssd"
        },
        {
            "op": "chooseleaf_firstn",
            "num": 0,
            "type": "host"
        },
        {
            "op": "emit"
        }
    ]
}
[root@ceph01 ~]# ceph osd getcrushmap -o crush.dump
131
[root@ceph01 ~]# crushtool -d crush.dump -o crush.cfg
[root@ceph01 ~]# vi crush.cfg
rule test-rule {
        id 2
        type replicated
        min_size 1
        max_size 10
        step take default class hdd
        step chooseleaf firstn 0 type host
        step emit
}
[root@ceph01 ~]# crushtool -c crush.cfg -o crush_new
[root@ceph01 ~]# ceph osd setcrushmap -i crush_new
132
[root@ceph01 ~]# ceph osd crush rule dump test-rule
{
    "rule_id": 2,
    "rule_name": "test-rule",
    "ruleset": 2,
    "type": 1,
    "min_size": 1,
    "max_size": 10,
    "steps": [
        {
            "op": "take",
            "item": -2,
            "item_name": "default~hdd"
        },
        {
            "op": "chooseleaf_firstn",
            "num": 0,
            "type": "host"
        },
        {
            "op": "emit"
        }
    ]
}
```

* 通过`ceph osd getcrushmap`命令将CRUSH MAP导出为二进制文件；
* 通过`crushtool`命令将导出的CRUSH MAP文件转换成文本，并编辑；
* 测试修改test-rule的ssd为hdd；
* 通过`crushtool`重新生成CRUSH MAP；
* `ceph osd setcrushmap`设置新的CRUSH MAP。