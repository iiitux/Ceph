# Ceph日常管理命令

## 监控集群

### 检查集群健康状态

```bash
[root@ceph01 ~]# ceph health
HEALTH_OK
```

### 观察集群

```bash
[root@ceph01 ~]# ceph -w
  cluster:
    id:     5882486a-47fc-4178-8e41-8e8d88bf2b9d
    health: HEALTH_OK

  services:
    mon: 3 daemons, quorum ceph01,ceph02,ceph03
    mgr: ceph01(active)
    osd: 3 osds: 3 up, 3 in

  data:
    pools:   1 pools, 128 pgs
    objects: 2 objects, 19 bytes
    usage:   3396 MB used, 27293 MB / 30690 MB avail
    pgs:     128 active+clean

2018-07-18 17:00:00.000572 mon.ceph01 [INF] overall HEALTH_OK
2018-07-18 17:01:44.511030 mon.ceph01 [INF] osd.1 marked itself down
2018-07-18 17:01:44.929668 mon.ceph01 [WRN] Health check failed: 1 osds down (OSD_DOWN)
2018-07-18 17:01:44.929821 mon.ceph01 [WRN] Health check failed: 1 host (1 osds) down (OSD_HOST_DOWN)
2018-07-18 17:01:44.929867 mon.ceph01 [WRN] Health check failed: 1 rack (1 osds) down (OSD_RACK_DOWN)
2018-07-18 17:01:47.973039 mon.ceph01 [WRN] Health check failed: Degraded data redundancy: 2/6 objects degraded (33.333%), 2 pgs degraded (PG_DEGRADED)
2018-07-18 17:01:54.937973 mon.ceph01 [INF] Health check cleared: OSD_DOWN (was: 1 osds down)
2018-07-18 17:01:54.938071 mon.ceph01 [INF] Health check cleared: OSD_HOST_DOWN (was: 1 host (1 osds) down)
2018-07-18 17:01:54.938112 mon.ceph01 [INF] Health check cleared: OSD_RACK_DOWN (was: 1 rack (1 osds) down)
2018-07-18 17:01:54.953935 mon.ceph01 [INF] osd.1 172.16.1.101:6800/1802 boot
2018-07-18 17:01:57.931049 mon.ceph01 [INF] Health check cleared: PG_DEGRADED (was: Degraded data redundancy: 2/6 objects degraded (33.333%), 2 pgs degraded)
2018-07-18 17:01:57.931157 mon.ceph01 [INF] Cluster is now healthy
```

### 检查集群的使用情况

```bash
[root@ceph01 ~]# ceph df
GLOBAL:
    SIZE       AVAIL      RAW USED     %RAW USED
    30690M     27293M        3396M         11.07
POOLS:
    NAME     ID     USED     %USED     MAX AVAIL     OBJECTS
    k8s      1        19         0         8586M           2
```

### 检查集群状态

```bash
[root@ceph01 ~]# ceph status
  cluster:
    id:     5882486a-47fc-4178-8e41-8e8d88bf2b9d
    health: HEALTH_OK

  services:
    mon: 3 daemons, quorum ceph01,ceph02,ceph03
    mgr: ceph01(active)
    osd: 3 osds: 3 up, 3 in

  data:
    pools:   1 pools, 128 pgs
    objects: 2 objects, 19 bytes
    usage:   3396 MB used, 27293 MB / 30690 MB avail
    pgs:     128 active+clean
```

### 检查monitor状态

```bash
[root@ceph01 ~]# ceph mon stat
e3: 3 mons at {ceph01=172.16.1.100:6789/0,ceph02=172.16.1.101:6789/0,ceph03=172.16.1.102:6789/0}, election epoch 76, leader 0 ceph01, quorum 0,1,2 ceph01,ceph02,ceph03

[root@ceph01 ~]# ceph mon dump
dumped monmap epoch 3
epoch 3
fsid 5882486a-47fc-4178-8e41-8e8d88bf2b9d
last_changed 2018-07-11 17:16:54.005244
created 2018-06-11 14:19:03.580533
0: 172.16.1.100:6789/0 mon.ceph01
1: 172.16.1.101:6789/0 mon.ceph02
2: 172.16.1.102:6789/0 mon.ceph03

[root@ceph01 ~]# ceph quorum_status
{
    "election_epoch": 76,
    "quorum": [
        0,
        1,
        2
    ],
    "quorum_names": [
        "ceph01",
        "ceph02",
        "ceph03"
    ],
    "quorum_leader_name": "ceph01",
    "monmap": {
        "epoch": 3,
        "fsid": "5882486a-47fc-4178-8e41-8e8d88bf2b9d",
        "modified": "2018-07-11 17:16:54.005244",
        "created": "2018-06-11 14:19:03.580533",
        "features": {
            "persistent": [
                "kraken",
                "luminous"
            ],
            "optional": []
        },
        "mons": [
            {
                "rank": 0,
                "name": "ceph01",
                "addr": "172.16.1.100:6789/0",
                "public_addr": "172.16.1.100:6789/0"
            },
            {
                "rank": 1,
                "name": "ceph02",
                "addr": "172.16.1.101:6789/0",
                "public_addr": "172.16.1.101:6789/0"
            },
            {
                "rank": 2,
                "name": "ceph03",
                "addr": "172.16.1.102:6789/0",
                "public_addr": "172.16.1.102:6789/0"
            }
        ]
    }
}

[root@ceph01 ~]# ceph mon_status
{
    "name": "ceph01",
    "rank": 0,
    "state": "leader",
    "election_epoch": 76,
    "quorum": [
        0,
        1,
        2
    ],
    "features": {
        "required_con": "153140804152475648",
        "required_mon": [
            "kraken",
            "luminous"
        ],
        "quorum_con": "2305244844532236283",
        "quorum_mon": [
            "kraken",
            "luminous"
        ]
    },
    "outside_quorum": [],
    "extra_probe_peers": [],
    "sync_provider": [],
    "monmap": {
        "epoch": 3,
        "fsid": "5882486a-47fc-4178-8e41-8e8d88bf2b9d",
        "modified": "2018-07-11 17:16:54.005244",
        "created": "2018-06-11 14:19:03.580533",
        "features": {
            "persistent": [
                "kraken",
                "luminous"
            ],
            "optional": []
        },
        "mons": [
            {
                "rank": 0,
                "name": "ceph01",
                "addr": "172.16.1.100:6789/0",
                "public_addr": "172.16.1.100:6789/0"
            },
            {
                "rank": 1,
                "name": "ceph02",
                "addr": "172.16.1.101:6789/0",
                "public_addr": "172.16.1.101:6789/0"
            },
            {
                "rank": 2,
                "name": "ceph03",
                "addr": "172.16.1.102:6789/0",
                "public_addr": "172.16.1.102:6789/0"
            }
        ]
    },
    "feature_map": {
        "mon": {
            "group": {
                "features": "0x1ffddff8eea4fffb",
                "release": "luminous",
                "num": 1
            }
        },
        "osd": {
            "group": {
                "features": "0x1ffddff8eea4fffb",
                "release": "luminous",
                "num": 3
            }
        },
        "client": {
            "group": {
                "features": "0x1ffddff8eea4fffb",
                "release": "luminous",
                "num": 2
            }
        }
    }
}
```

### 检查osd状态信息

```bash
[root@ceph01 ~]# ceph osd stat
3 osds: 3 up, 3 in

[root@ceph01 ~]# ceph osd dump
epoch 89
fsid 5882486a-47fc-4178-8e41-8e8d88bf2b9d
created 2018-06-11 14:37:17.176439
modified 2018-07-18 17:01:55.950531
flags sortbitwise,recovery_deletes,purged_snapdirs
crush_version 19
full_ratio 0.95
backfillfull_ratio 0.9
nearfull_ratio 0.85
require_min_compat_client jewel
min_compat_client jewel
require_osd_release luminous
pool 1 'k8s' replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 last_change 74 flags hashpspool stripe_width 0 application rgw
    removed_snaps [1~3]
max_osd 3
osd.0 up   in  weight 1 up_from 83 up_thru 88 down_at 79 last_clean_interval [76,78) 172.16.1.100:6800/1066 172.16.2.100:6800/1066 172.16.2.100:6801/1066 172.16.1.100:6801/1066 exists,up b19d57c7-de6a-415e-8fcd-75b76586b30b
osd.1 up   in  weight 1 up_from 88 up_thru 88 down_at 86 last_clean_interval [83,85) 172.16.1.101:6800/1802 172.16.2.101:6800/1802 172.16.2.101:6801/1802 172.16.1.101:6801/1802 exists,up be112c37-a71c-4f34-b21b-8965437a20c2
osd.2 up   in  weight 1 up_from 84 up_thru 88 down_at 83 last_clean_interval [77,82) 172.16.1.102:6800/1065 172.16.2.102:6800/1065 172.16.2.102:6801/1065 172.16.1.102:6801/1065 exists,up 397ada91-d660-4b50-8076-3a53af49e753

[root@ceph01 ~]# ceph osd tree
ID  CLASS WEIGHT  TYPE NAME           STATUS REWEIGHT PRI-AFF
 -1       0.02939 root default
 -9       0.00980     rack rack01
 -3       0.00980         host ceph01
  0   hdd 0.00980             osd.0       up  1.00000 1.00000
-10       0.00980     rack rack02
 -5       0.00980         host ceph02
  1   hdd 0.00980             osd.1       up  1.00000 1.00000
-11       0.00980     rack rack03
 -7       0.00980         host ceph03
  2   hdd 0.00980             osd.2       up  1.00000 1.00000
```

### 检查mds状态

```bash
[root@ceph01 ~]# ceph mds stat
, 1 up:standby
[root@ceph01 ~]# ceph mds dump
dumped fsmap epoch 2
fs_name cephfs
epoch   2
flags   c
created 0.000000
modified    0.000000
tableserver 0
root    0
session_timeout 0
session_autoclose   0
max_file_size   0
last_failure    0
last_failure_osd_epoch  0
compat  compat={},rocompat={},incompat={}
max_mds 0
in
up  {}
failed
damaged
stopped
data_pools  []
metadata_pool   -1
inline_data disabled
balancer
standby_count_wanted    0
```

## 监控OSD和PG

### 检查osd状态

```bash
[root@ceph01 ~]# ceph osd stat
3 osds: 3 up, 3 in
```

* `in`和`out`表示是否在集群内；
* `up`和`down`表示osd服务是否运行；

```bash
[root@ceph01 ~]# ceph osd tree
ID  CLASS WEIGHT  TYPE NAME           STATUS REWEIGHT PRI-AFF
 -1       0.02939 root default
 -9       0.00980     rack rack01
 -3       0.00980         host ceph01
  0   hdd 0.00980             osd.0       up  1.00000 1.00000
-10       0.00980     rack rack02
 -5       0.00980         host ceph02
  1   hdd 0.00980             osd.1       up  1.00000 1.00000
-11       0.00980     rack rack03
 -7       0.00980         host ceph03
  2   hdd 0.00980             osd.2       up  1.00000 1.00000
```

### 检查PG

```bash
[root@ceph01 ~]# ceph pg map 1.5f
osdmap e100 pg 1.5f (1.5f) -> up [0,2,1] acting [0,2,1]
```

* `e100`为osdmap版本号；
* `1.5f`为pg号，其中`1`为`pool num`，`5f`为`pg id`；
* `[0.2.1]`为up set内的osd；

```bash
[root@ceph01 ~]# ceph pg stat
128 pgs: 128 active+clean; 19 bytes data, 3396 MB used, 27293 MB / 30690 MB avail
```

```bash
[root@ceph01 ~]# ceph pg dump
dumped all
version 4215
stamp 2018-07-20 11:26:47.339032
last_osdmap_epoch 0
last_pg_scan 0
full_ratio 0
nearfull_ratio 0
......
......

1 2 0 0 0 0 19 31 31

sum 2 0 0 0 0 19 31 31
OSD_STAT USED  AVAIL  TOTAL  HB_PEERS PG_SUM PRIMARY_PG_SUM
2        1132M  9097M 10230M    [0,1]    128             39
1        1132M  9097M 10230M    [0,2]    128             46
0        1132M  9097M 10230M    [1,2]    128             43
sum      3396M 27293M 30690M
```

```bash
[root@ceph01 ~]# ceph pg dump -o pg.json --format=json
dumped all
```

```bash
[root@ceph01 ~]# ceph pg 1.5f query
```